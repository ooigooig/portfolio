---
layout: post
title: Neural Networks note
date: 2024-08-23 16:40:16
description: 
tags: neural_networks
categories: note
giscus_comments: true
---

**1.Is KL divergence same as cross entropy for image classification?**

**<mark><u>Yes</u></mark>**

In Image classification, we use one-hot encoding for our labels. Therefore, when y_i is the actual label, it equals 1 â†’ log (1) = 0, and the whole term is cancelled. When y_i is not the correct label, it equals 0 and the whole term is also cancelled out.

Therefore, KL divergence = Cross Entropy in image classification tasks

**2.Why cross entropy comes in hand with Softmax layer?**

**<mark><u>Why we need to use softmax function after cross entropy?</u></mark>**

Because thecross entropy loss takes the *logatithm of the probability*. So in order to compute an efficient logarithm, we need to have *a probability distribution that sums up to 1*.

**3.Contractive loss**

- Contrastive Loss is a distance-based Loss function (as opposed to **prediction error-based** losses like cross entropy) used to learn discriminative features for images.
- Like any distance-based loss, it tries to ensure that semantically similar examples are embedded close together. It is calculated on **Pairs**
- This loss measures the *<mark>similarity</mark>* between two inputs.
- Each sample is composed of two images (*<mark>positive pairs</mark>* or *<mark>negative pairs</mark>*). Our goal is to maximize the distance between *<mark>negative pairs</mark>* and <u>minimize</u> the distance between <mark>*positive pairs*</mark>.
- We want small distance between the positive pairs (because they are similar images/inputs), and great distance than some margin m for negative pairs.